# -*- coding: utf-8 -*-
"""Assignment2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1au4FM1ir0Sh5rBHzCO4HojZOEhX25CoK

##Named Entity Recognition (NER) using a classifier
"""

import pandas as pd
import numpy as np

import spacy
nlp = spacy.load('en_core_web_sm')

from imblearn.over_sampling import SMOTE
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix

"""### Import the training and development data"""

wnuttrain = 'https://storage.googleapis.com/wnut-2017_ner-shared-task/wnut17train_clean_tagged.txt'
train = pd.read_table(wnuttrain, header=None, names=['token', 'label', 'bio_only', 'upos'])

wnutdev = 'https://storage.googleapis.com/wnut-2017_ner-shared-task/wnut17dev_clean_tagged.txt'
dev = pd.read_table(wnutdev, header=None, names=['token', 'label', 'bio_only', 'upos'])

"""###Devise function to exctract all suitable features and store them in copies of the training and development dataframes"""

# in order to convert POS tags to integers: get the UPOS tagset
pos_vocab = train.upos.unique().tolist()

# feature 1: convert POS-tags to integers
def pos_index(pos):
  ind = pos_vocab.index(pos)
  return ind

# feature 2: is this a proper noun?
def is_propn(pos):
  resp = False
  if pos=='PROPN':
    resp = True
  return resp

#  feature 3: is the first character a capital letter?
def title_case(tok):
  resp = False
  if tok[0].isupper():
    resp = True  # thanks Archie Barrett for spotting a typo here!
  return resp

# feature 4: token length
def tok_len(tok):
  tok_len = len(tok)
  return tok_len

# features 5, 6, 7 extracted after token i s turned into a spacy token:  
  # spacy tag (extended pos tagging)
  # is the token comprise of alphabetical characters only
  # is the token a stop word
def get_spacy_doc(t):
  ret = ''
  doc = nlp(t)
  for w in doc:
    ret = w
  return ret

# training labels: convert BIO to integers
def bio_index(bio):
  ind = 10
  if bio=='B':
    ind = 0
  elif bio=='I':
    ind = 1
  elif bio=='O':
    ind = 2
  return ind

# pass a data frame through our feature extractor
def extract_features(txt_orig, bio=True):
  txt = txt_orig.copy()
  txt = txt.dropna()

  txt['pos_indices'] = [pos_index(u) for u in txt['upos']]
  txt['is_propn'] = [is_propn(u) for u in txt['upos']]
  txt['title_case'] = [title_case(str(t)) for t in txt['token']]
  txt['token_length'] = [tok_len(str(t)) for t in txt['token']]
  txt['docs'] = [get_spacy_doc(str(t)) for t in txt['token']]
  txt['spacy_tag'] = [t.tag for t in txt['docs']]
  txt['is_alpha'] = [t.is_alpha for t in txt['docs']]
  txt['is_stop'] = [t.is_stop for t in txt['docs']]

  if bio==True:
    bioints = [bio_index(b) for b in txt['bio_only']]
    txt['bio_only'] = bioints

  return txt

"""###Extract the features for each dataset"""

train_copy = extract_features(train)

dev_copy = extract_features(dev)

"""###Prepare the data for training: separate into feature vectors (X) and labels (y).
Use oversampling for the training data to account for the significantly larger number of O labels
"""

smote = SMOTE(sampling_strategy='auto')

X_train = train_copy.drop(['token', 'label', 'bio_only', 'upos', 'docs'], axis=1)
y_train = train_copy['bio_only']

X_train, y_train = smote.fit_sample(X_train, y_train)

X_dev = dev_copy.drop(['token', 'label', 'bio_only', 'upos', 'docs'], axis=1)
y_dev = dev_copy['bio_only']

"""###Train the Random Forest Classifier using the training feature vector and labels"""

rfc = RandomForestClassifier().fit(X_train, y_train)

"""Function to translate the integers 0, 1, 2 to B, I, O"""

def index_to_bio(ind):
  bio='M'
  if ind==0:
    bio = 'B'
  elif ind==1:
    bio = 'I'
  elif ind==2:
    bio = 'O'
  return bio

"""###Get the predictions for the development set from the trained model"""

preds = rfc.predict(X_dev)
preds_bio = [index_to_bio(i) for i in preds]
dev_copy['prediction'] = preds_bio

"""###Compare the number of different values in the golden stanadard label set and the predicted label set. 
B corresponds to 0, I to 1 and O to 2
"""

y_dev.value_counts()

dev_copy['prediction'].value_counts()

"""###Analyse the classification report and the confusion matrix"""

print(classification_report(y_dev, preds ))

"""i-th row and j-th column entry indicates the number of samples with true label being i-th class and prediced label being j-th class.

```
   B   I   O
   0   1   2
0  tp  ?   fn    
1  ?   tp  fn
2  fp  fp  tn
```
"""

confusion_matrix(y_dev, preds, labels=[0, 1, 2])

"""### Import the testing data"""

wnuttest = 'https://storage.googleapis.com/wnut-2017_ner-shared-task/wnut17test_clean_tagged.txt'
testset = pd.read_table(wnuttest, header=None, names=['token', 'upos']).dropna()

"""### Extract the features of the testing data"""

test_copy = extract_features(testset, bio=False)

"""###Prepare the test feature vector and use it with the defined RFC model to get predictions"""

X_test = test_copy.drop(['token', 'upos', 'docs'], axis=1)
test_preds = rfc.predict(X_test)
test_preds_bio = [index_to_bio(i) for i in test_preds]
test_copy['prediction'] = test_preds_bio

test_copy.head(2)

"""### Drop the feature columns and export the test data with the predictions"""

test_result = test_copy.drop(['pos_indices',	'is_propn', 'title_case',	'token_length',	'docs',	'spacy_tag',	'is_alpha',	'is_stop'], axis=1)
test_result.head(2)

from google.colab import drive
drive.mount('drive')

test_result.to_csv('drive/My Drive/Colab Notebooks/testset_mmm67.csv', index=False)